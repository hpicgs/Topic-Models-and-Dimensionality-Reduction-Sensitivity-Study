{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f16f1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/datasets/thedevastator/new-dataset-for-text-classification-ag-news?resource=download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccc3848a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel Atzberger\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.4)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n",
      "C:\\Users\\Daniel Atzberger\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "import spacy\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from','subject','re','edu','use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eee1541a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reute...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Wall St. Bears Claw Back Into the Black (Reute...      2\n",
       "1  Carlyle Looks Toward Commercial Aerospace (Reu...      2\n",
       "2  Oil and Economy Cloud Stocks' Outlook (Reuters...      2\n",
       "3  Iraq Halts Oil Exports from Main Southern Pipe...      2\n",
       "4  Oil prices soar to all-time record, posing new...      2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/corpus_agnews_train.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea3400d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df[\"label\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87791a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in tqdm(sentences):\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "facba7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d37e0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in tqdm(texts):\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8c59351",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 120000/120000 [00:14<00:00, 8130.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start removing stop words\n",
      "Installing spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                             | 19/120000 [00:00<11:19, 176.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start lemmatizing words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 120000/120000 [06:30<00:00, 306.92it/s]\n"
     ]
    }
   ],
   "source": [
    "data = df[\"text\"].tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "# Remove Stop Words\n",
    "print(\"Start removing stop words\")\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# conda install -c conda-forge spacy-model-en_core_web_sm\n",
    "print(\"Installing spacy\")\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "print(\"Start lemmatizing words\")\n",
    "#data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "data_lemmatized = lemmatization(data_words_nostops, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6011f91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n",
      "['wall', 'st', 'bear', 'claw', 'back', 'black', 'reuter', 'reuter', 'short', 'seller', 'wall', 'street', 'dwindle', 'band', 'ultra', 'cynic', 'see', 'green']\n"
     ]
    }
   ],
   "source": [
    "print(df[\"text\"].tolist()[0])\n",
    "print(data_lemmatized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25579817",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 120000/120000 [00:00<00:00, 243830.90it/s]\n"
     ]
    }
   ],
   "source": [
    "data_lemmatized_min_length = []\n",
    "\n",
    "for sublist in tqdm(data_lemmatized):\n",
    "    # Use a list comprehension to filter out strings with less than two characters\n",
    "    sublist = [word for word in sublist if len(word) > 2]\n",
    "    data_lemmatized_min_length.append(sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29564ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n",
      "['wall', 'st', 'bear', 'claw', 'back', 'black', 'reuter', 'reuter', 'short', 'seller', 'wall', 'street', 'dwindle', 'band', 'ultra', 'cynic', 'see', 'green']\n",
      "['wall', 'bear', 'claw', 'back', 'black', 'reuter', 'reuter', 'short', 'seller', 'wall', 'street', 'dwindle', 'band', 'ultra', 'cynic', 'see', 'green']\n"
     ]
    }
   ],
   "source": [
    "print(df[\"text\"].tolist()[0])\n",
    "print(data_lemmatized[0])\n",
    "print(data_lemmatized_min_length[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0eee438c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 2), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 2)]]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized_min_length)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized_min_length\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View \n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8614463f",
   "metadata": {},
   "source": [
    "### Topic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b406595d",
   "metadata": {},
   "source": [
    "#### Vector Space Model (VSM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bb271b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import dok_matrix\n",
    "\n",
    "# Define function to convert Gensim corpus to a sparse pandas DataFrame\n",
    "def corpus_to_sparse_dataframe(corpus):\n",
    "    word_freq = dok_matrix((len(corpus), len(id2word)), dtype=int)\n",
    "\n",
    "    for i, doc in enumerate(corpus):\n",
    "        for word_id, freq in doc:\n",
    "            word_freq[i, word_id] = freq\n",
    "\n",
    "    dataframe = pd.DataFrame.sparse.from_spmatrix(word_freq)\n",
    "    dataframe.columns = [id2word[word_id] for word_id in range(len(id2word))]\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af0162a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "VSM = corpus_to_sparse_dataframe(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4480ebfd",
   "metadata": {},
   "source": [
    "#### Vector Space Model & tf-idf (VSM & tf-idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1c34a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel\n",
    "\n",
    "model = TfidfModel(corpus)  # fit model\n",
    "tfidf_corpus = model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34ca7bd",
   "metadata": {},
   "source": [
    "#### Latent Semantic Indexing (LSI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8fccacaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.404*\"reuter\" + 0.378*\"say\" + 0.284*\"new\" + 0.174*\"stock\" + 0.163*\"fullquote\" + 0.139*\"year\" + 0.117*\"york\" + 0.113*\"oil\" + 0.111*\"com\" + 0.110*\"company\"'),\n",
       " (1,\n",
       "  '-0.365*\"reuter\" + 0.338*\"say\" + -0.325*\"fullquote\" + -0.262*\"stock\" + -0.184*\"investor\" + -0.173*\"http\" + -0.172*\"href\" + -0.172*\"www\" + -0.171*\"ticker\" + 0.168*\"quot\"'),\n",
       " (2,\n",
       "  '0.624*\"new\" + -0.504*\"say\" + -0.226*\"reuter\" + 0.189*\"york\" + 0.132*\"oil\" + -0.132*\"kill\" + 0.117*\"price\" + -0.110*\"iraq\" + 0.104*\"game\" + 0.082*\"high\"'),\n",
       " (3,\n",
       "  '0.559*\"oil\" + 0.408*\"price\" + -0.337*\"new\" + -0.240*\"quot\" + 0.193*\"high\" + 0.136*\"rise\" + -0.128*\"fullquote\" + 0.119*\"year\" + 0.112*\"crude\" + 0.098*\"fall\"'),\n",
       " (4,\n",
       "  '-0.361*\"quot\" + -0.351*\"say\" + -0.332*\"new\" + 0.251*\"year\" + -0.218*\"oil\" + 0.215*\"game\" + 0.205*\"first\" + 0.177*\"win\" + 0.175*\"two\" + 0.172*\"reuter\"'),\n",
       " (5,\n",
       "  '0.667*\"quot\" + -0.396*\"new\" + -0.214*\"reuter\" + 0.152*\"year\" + -0.150*\"say\" + -0.145*\"kill\" + 0.128*\"company\" + -0.119*\"york\" + 0.114*\"microsoft\" + -0.108*\"iraq\"'),\n",
       " (6,\n",
       "  '0.477*\"quot\" + -0.261*\"company\" + -0.205*\"microsoft\" + 0.172*\"oil\" + -0.170*\"inc\" + 0.156*\"iraq\" + 0.150*\"game\" + -0.143*\"year\" + 0.139*\"reuter\" + -0.137*\"say\"'),\n",
       " (7,\n",
       "  '-0.348*\"kill\" + 0.290*\"say\" + -0.242*\"iraq\" + 0.200*\"olympic\" + -0.192*\"microsoft\" + -0.166*\"two\" + -0.151*\"palestinian\" + -0.145*\"gaza\" + 0.144*\"athen\" + 0.141*\"gold\"')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import LsiModel\n",
    "\n",
    "K = 8\n",
    "lsi_model = LsiModel(corpus, id2word=id2word, num_topics=K)\n",
    "lsi_model.print_topics(num_topics=K, num_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6440a9",
   "metadata": {},
   "source": [
    "#### Latent Semantic Indexing & tf-idf (LSI & tf-idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adbcc723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.201*\"reuter\" + 0.176*\"oil\" + 0.151*\"stock\" + 0.149*\"new\" + 0.147*\"say\" + 0.142*\"price\" + 0.108*\"year\" + 0.104*\"high\" + 0.096*\"fullquote\" + 0.095*\"rise\"'),\n",
       " (1,\n",
       "  '-0.289*\"oil\" + -0.280*\"stock\" + -0.225*\"fullquote\" + -0.218*\"price\" + 0.156*\"kill\" + -0.147*\"investor\" + 0.138*\"iraq\" + -0.138*\"reuter\" + -0.119*\"profit\" + -0.114*\"rise\"'),\n",
       " (2,\n",
       "  '0.449*\"oil\" + 0.269*\"price\" + -0.197*\"microsoft\" + -0.179*\"fullquote\" + 0.156*\"iraq\" + 0.143*\"crude\" + 0.136*\"kill\" + -0.122*\"com\" + 0.117*\"barrel\" + -0.115*\"software\"'),\n",
       " (3,\n",
       "  '-0.303*\"fullquote\" + -0.250*\"kill\" + -0.210*\"reuter\" + 0.196*\"oil\" + -0.181*\"iraq\" + -0.152*\"quickinfo\" + -0.152*\"aspx\" + -0.151*\"href\" + -0.151*\"http\" + -0.150*\"baghdad\"'),\n",
       " (4,\n",
       "  '-0.320*\"microsoft\" + 0.210*\"game\" + 0.157*\"sox\" + 0.155*\"win\" + -0.152*\"search\" + 0.143*\"red\" + -0.137*\"window\" + -0.136*\"software\" + -0.128*\"google\" + 0.114*\"series\"'),\n",
       " (5,\n",
       "  '-0.237*\"olympic\" + -0.195*\"athen\" + 0.190*\"bush\" + -0.187*\"microsoft\" + -0.186*\"gold\" + -0.160*\"medal\" + -0.154*\"kill\" + 0.151*\"president\" + 0.143*\"election\" + 0.137*\"iran\"'),\n",
       " (6,\n",
       "  '-0.366*\"olympic\" + -0.300*\"athen\" + -0.295*\"gold\" + -0.273*\"arafat\" + -0.256*\"palestinian\" + -0.247*\"medal\" + 0.234*\"najaf\" + 0.151*\"iraq\" + -0.142*\"yasser\" + 0.119*\"sox\"'),\n",
       " (7,\n",
       "  '0.357*\"arafat\" + 0.317*\"palestinian\" + 0.239*\"hurricane\" + -0.225*\"olympic\" + 0.218*\"ivan\" + -0.190*\"gold\" + 0.185*\"yasser\" + -0.184*\"athen\" + -0.172*\"najaf\" + -0.167*\"iraq\"')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import LsiModel\n",
    "\n",
    "K = 8\n",
    "tfidf_lsi_model = LsiModel(tfidf_corpus, id2word=id2word, num_topics=K)\n",
    "tfidf_lsi_model.print_topics(num_topics=K, num_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3245adcb",
   "metadata": {},
   "source": [
    "#### Non-Negative Matrix Factorization (NMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "806e7577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.027*\"year\" + 0.010*\"two\" + 0.009*\"one\" + 0.008*\"first\" + 0.007*\"last\" + 0.006*\"kill\" + 0.006*\"three\" + 0.005*\"week\" + 0.005*\"time\" + 0.005*\"report\"'),\n",
       " (1,\n",
       "  '0.046*\"quot\" + 0.041*\"game\" + 0.013*\"state\" + 0.011*\"reuter\" + 0.008*\"team\" + 0.007*\"united\" + 0.007*\"olympic\" + 0.007*\"world\" + 0.006*\"athen\" + 0.005*\"play\"'),\n",
       " (2,\n",
       "  '0.065*\"oil\" + 0.050*\"price\" + 0.026*\"high\" + 0.023*\"stock\" + 0.017*\"rise\" + 0.013*\"crude\" + 0.013*\"fall\" + 0.010*\"barrel\" + 0.010*\"record\" + 0.010*\"low\"'),\n",
       " (3,\n",
       "  '0.092*\"new\" + 0.025*\"york\" + 0.004*\"search\" + 0.004*\"yankee\" + 0.004*\"service\" + 0.004*\"announce\" + 0.003*\"launch\" + 0.003*\"system\" + 0.003*\"microsoft\" + 0.003*\"technology\"'),\n",
       " (4,\n",
       "  '0.042*\"say\" + 0.015*\"company\" + 0.013*\"reuter\" + 0.009*\"microsoft\" + 0.009*\"inc\" + 0.008*\"corp\" + 0.007*\"plan\" + 0.007*\"million\" + 0.007*\"service\" + 0.006*\"software\"'),\n",
       " (5,\n",
       "  '0.034*\"iraq\" + 0.034*\"say\" + 0.018*\"kill\" + 0.015*\"iraqi\" + 0.012*\"baghdad\" + 0.010*\"quot\" + 0.009*\"official\" + 0.008*\"afp\" + 0.008*\"force\" + 0.008*\"attack\"'),\n",
       " (6,\n",
       "  '0.035*\"win\" + 0.020*\"gold\" + 0.018*\"first\" + 0.015*\"olympic\" + 0.012*\"medal\" + 0.011*\"world\" + 0.009*\"two\" + 0.009*\"lead\" + 0.009*\"victory\" + 0.008*\"second\"'),\n",
       " (7,\n",
       "  '0.081*\"reuter\" + 0.062*\"fullquote\" + 0.043*\"stock\" + 0.035*\"com\" + 0.034*\"investor\" + 0.034*\"http\" + 0.033*\"www\" + 0.033*\"href\" + 0.033*\"ticker\" + 0.033*\"target\"')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/nmf_tutorial.ipynb\n",
    "from gensim.models.nmf import Nmf\n",
    "\n",
    "K = 8\n",
    "nmf_model = Nmf(corpus, id2word=id2word, num_topics=K)\n",
    "nmf_model.show_topics(num_topics=K, num_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b6a6d7",
   "metadata": {},
   "source": [
    "#### Non-Negative Matrix Factorization & tf-idf (NMF & tf-idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9263adda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.048*\"google\" + 0.035*\"search\" + 0.016*\"share\" + 0.013*\"engine\" + 0.011*\"inc\" + 0.010*\"public\" + 0.010*\"yahoo\" + 0.010*\"web\" + 0.009*\"internet\" + 0.008*\"desktop\"'),\n",
       " (1,\n",
       "  '0.036*\"fullquote\" + 0.028*\"reuter\" + 0.025*\"stock\" + 0.019*\"investor\" + 0.019*\"http\" + 0.019*\"href\" + 0.018*\"www\" + 0.018*\"aspx\" + 0.018*\"quickinfo\" + 0.018*\"ticker\"'),\n",
       " (2,\n",
       "  '0.057*\"oil\" + 0.038*\"price\" + 0.018*\"crude\" + 0.017*\"high\" + 0.015*\"barrel\" + 0.014*\"stock\" + 0.012*\"supply\" + 0.010*\"rise\" + 0.010*\"fall\" + 0.010*\"record\"'),\n",
       " (3,\n",
       "  '0.021*\"iraq\" + 0.019*\"kill\" + 0.015*\"iraqi\" + 0.014*\"baghdad\" + 0.008*\"bomb\" + 0.008*\"najaf\" + 0.008*\"militant\" + 0.007*\"attack\" + 0.007*\"troop\" + 0.007*\"hostage\"'),\n",
       " (4,\n",
       "  '0.035*\"gold\" + 0.023*\"olympic\" + 0.023*\"medal\" + 0.016*\"athen\" + 0.009*\"woman\" + 0.008*\"hamm\" + 0.008*\"phelp\" + 0.007*\"gaza\" + 0.006*\"man\" + 0.006*\"palestinian\"'),\n",
       " (5,\n",
       "  '0.013*\"game\" + 0.007*\"sox\" + 0.007*\"red\" + 0.006*\"win\" + 0.006*\"series\" + 0.005*\"team\" + 0.005*\"first\" + 0.005*\"league\" + 0.005*\"season\" + 0.005*\"run\"'),\n",
       " (6,\n",
       "  '0.006*\"quot\" + 0.005*\"say\" + 0.004*\"year\" + 0.003*\"bush\" + 0.003*\"report\" + 0.003*\"president\" + 0.003*\"state\" + 0.003*\"afp\" + 0.003*\"china\" + 0.003*\"sale\"'),\n",
       " (7,\n",
       "  '0.021*\"microsoft\" + 0.010*\"software\" + 0.009*\"window\" + 0.007*\"service\" + 0.006*\"system\" + 0.006*\"security\" + 0.005*\"corp\" + 0.005*\"computer\" + 0.005*\"company\" + 0.005*\"new\"')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/nmf_tutorial.ipynb\n",
    "from gensim.models.nmf import Nmf\n",
    "\n",
    "K = 8\n",
    "tfidf_nmf_model = Nmf(tfidf_corpus, id2word=id2word, num_topics=K)\n",
    "tfidf_nmf_model.show_topics(num_topics=K, num_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e674fdd",
   "metadata": {},
   "source": [
    "#### Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "885cf740",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\DANIEL~1\\AppData\\Local\\Temp/ipykernel_4952/1841929072.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Build LDA model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mK\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n\u001b[0m\u001b[0;32m      4\u001b[0m                                            \u001b[0mid2word\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mid2word\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                                            \u001b[0mnum_topics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[0;32m    521\u001b[0m             \u001b[0muse_numpy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatcher\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m             \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 523\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunks_as_numpy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_numpy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    524\u001b[0m             self.add_lifecycle_event(\n\u001b[0;32m    525\u001b[0m                 \u001b[1;34m\"created\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, corpus, chunksize, decay, offset, passes, update_every, eval_every, iterations, gamma_threshold, chunks_as_numpy)\u001b[0m\n\u001b[0;32m   1006\u001b[0m                         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"reached the end of input; now waiting for all remaining jobs to finish\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1007\u001b[0m                         \u001b[0mother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetstate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1008\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrho\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpass_\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1009\u001b[0m                     \u001b[1;32mdel\u001b[0m \u001b[0mother\u001b[0m  \u001b[1;31m# frees up memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001b[0m in \u001b[0;36mdo_mstep\u001b[1;34m(self, rho, other, extra_pass)\u001b[0m\n\u001b[0;32m   1055\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrho\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1056\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1057\u001b[1;33m         \u001b[0mcurrent_Elogbeta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_Elogbeta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1058\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msync_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_Elogbeta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1059\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Build LDA model\n",
    "K = 8\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=K,\n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=400,\n",
    "                                           passes=30,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "# Print the Keywords in the 3 topics\n",
    "pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebcca6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
