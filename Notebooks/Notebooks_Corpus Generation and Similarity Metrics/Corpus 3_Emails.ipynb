{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd477bf5",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b02d9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/datasets/dipankarsrirag/topic-modelling-on-emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "365e03b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel Atzberger\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.4)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n",
      "C:\\Users\\Daniel Atzberger\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "import spacy\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from','subject','re','edu','use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b2355ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_txt_files(directory):\n",
    "    txt_files = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                txt_files.append(os.path.join(root, file))\n",
    "    return txt_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "911e83b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding=\"windows-1252\") as file:\n",
    "            file_contents = file.read()\n",
    "        return file_contents\n",
    "    except FileNotFoundError:\n",
    "        return \"File not found.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3313be8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in tqdm(sentences):\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adc2d108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09561046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in tqdm(texts):\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7134fb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# category 1: Crime\n",
    "directory_path = \"C:\\\\Users\\\\Daniel Atzberger\\\\Documents\\\\IEEE_Vis24\\\\data\\\\Emails\\\\Crime\"\n",
    "file_list = find_txt_files(directory_path)\n",
    "\n",
    "file_contents_Crime = []\n",
    "labels_Crime = []\n",
    "for file in file_list:\n",
    "    file_contents_Crime.append(read_txt_file(file))\n",
    "    labels_Crime.append(\"Crime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13723907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# category 2: Entertainment\n",
    "directory_path = \"C:\\\\Users\\\\Daniel Atzberger\\\\Documents\\\\IEEE_Vis24\\\\data\\\\Emails\\\\Entertainment\"\n",
    "file_list = find_txt_files(directory_path)\n",
    "\n",
    "file_contents_Entertainment = []\n",
    "labels_Entertainment = []\n",
    "for file in file_list:\n",
    "    file_contents_Entertainment.append(read_txt_file(file))\n",
    "    labels_Entertainment.append(\"Entertainment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d3862d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# category 3: Politics\n",
    "directory_path = \"C:\\\\Users\\\\Daniel Atzberger\\\\Documents\\\\IEEE_Vis24\\\\data\\\\Emails\\\\Politics\"\n",
    "file_list = find_txt_files(directory_path)\n",
    "\n",
    "file_contents_Politics = []\n",
    "labels_Politics = []\n",
    "for file in file_list:\n",
    "    file_contents_Politics.append(read_txt_file(file))\n",
    "    labels_Politics.append(\"Politics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03f81c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# category 4: Science\n",
    "directory_path = \"C:\\\\Users\\\\Daniel Atzberger\\\\Documents\\\\IEEE_Vis24\\\\data\\\\Emails\\\\Science\"\n",
    "file_list = find_txt_files(directory_path)\n",
    "\n",
    "file_contents_Science = []\n",
    "labels_Science = []\n",
    "for file in file_list:\n",
    "    file_contents_Science.append(read_txt_file(file))\n",
    "    labels_Science.append(\"Science\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd098e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 9156/9156 [00:08<00:00, 1071.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start removing stop words\n",
      "Installing spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                 | 1/9156 [00:00<19:46,  7.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start lemmatizing words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 9156/9156 [01:52<00:00, 81.71it/s]\n"
     ]
    }
   ],
   "source": [
    "data = file_contents_Crime + file_contents_Entertainment + file_contents_Politics + file_contents_Science\n",
    "labels = labels_Crime + labels_Entertainment + labels_Politics + labels_Science\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "# Remove Stop Words\n",
    "print(\"Start removing stop words\")\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# conda install -c conda-forge spacy-model-en_core_web_sm\n",
    "print(\"Installing spacy\")\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "print(\"Start lemmatizing words\")\n",
    "#data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "data_lemmatized = lemmatization(data_words_nostops, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4721d010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['distribution', 'na', 'message', 'i', 'd', 'rs', 'au', 'access', 'digex', 'net', 'reference', 'apr', 'kelvin', 'jpl', 'nasa', 'gov', 'apr', 'stsci', 'rrhlo', 'ajb', 'access', 'digex', 'net', 'apr', 'stsci', 'nntp', 'post', 'host', 'access', 'digex', 'net', 'article', 'apr', 'stsci', 'hathaway', 'stsci', 'write', 'space', 'walk', 'go', 'boost', 'hst', 'orbit', 'think', 'right', 'sit', 'mile', 'would', 'like', 'know', 'exact', 'orbit', 'number', 'ben', 'say', 'boost', 'idea', 'news', 'we', 'know', 'something', 'please', 'supply', 'source', 'would', 'nice', 'scheduler', 'observation', 'know', 'thing', 'go', 'altitude', 'number', 'also', 'way', 'good', 'source', 'minimum', 'st', 'altitude', 'pmdb', 'kilometer', 'maximum', 'st', 'altitude', 'pmdb', 'kilometer', 'delta', 'st', 'altitude', 'pmdb', 'kilometer', 'pmdb', 'proposal', 'management', 'datum', 'base', 'use', 'schedule', 'observation', 'sure', 'number', 'far', 'well', 'mine', 'say', 'exact', 'number', 'order', 'perform', 'boost', 'hst', 'oms', 'engine', 'fire', 'long', 'period', 'shuttle', 'heavy', 'thing', 'hst', 'light', 'either', 'amount', 'oms', 'fuel', 'need', 'fly', 'substantial', 'small', 'booster', 'carry', 'use', 'boost', 'hst', 'weigh', 'significantly', 'less', 'oms', 'fuel', 'require', 'boost', 'hst', 'shuttle', 'give', 'orbital', 'change', 'could', 'supply', 'calculation', 'might', 'check', 'recent', 'posting', 'explain', 'small', 'booster', 'suggest', 'exist', 'compare', 'mass', 'something', 'exist', 'mass', 'oms', 'fuel', 'seem', 'impossible', 'contamination', 'threat', 'also', 'remain', 'different', 'would', 'contamination', 'threat', 'small', 'manuevere', 'tug', 'shuttle', 'om', 'engine', 'know', 'small', 'manuevere', 'tug', 'exist', 'maybe', 'one', 'could', 'soup', 'bus', 'anyone', 'de', 'clasifie', 'spec', 'hte', 'bus', 'would', 'able', 'provide', 'enough', 'control', 'force', 'balance', 'hst', 'still', 'rocket', 'thrust', 'hurl', 'decent', 'high', 'orbit', 'hst', 'could', 'even', 'get', 'place', 'sort', 'medium', 'orbit', 'reason', 'want', 'high', 'orbit', 'less', 'antenna', 'pointing', 'long', 'drag', 'life', 'long', 'drag', 'life', 'understand', 'could', 'explain', 'antenna', 'pointing', 'sorry', 'intrument', 'pointing', 'whatever', 'problem', 'tilt', 'array', 'big', 'constraint', 'hst', 'op', 'tell', 'although', 'array', 'move', 'perfectly', 'well', 'utilize', 'second', 'electronic', 'box', 'get', 'work', 'much', 'desireable', 'reclaim', 'redundancy', 'plus', 'second', 'box', 'get', 'fritzy', 'could', 'shitter', 'ville', 'real', 'fast', 'mean', 'jump', 'helpful', 'suggestion', 'always', 'welcome', 'know', 'idea', 'well', 'want', 'true', 'situation', 'describe', 'clearly', 'correctly', 'lest', 'get', 'confused', 'regard', 'wm', 'hathaway', 'problem', 'one', 'seem', 'exact', 'number', 'mission', 'plan', 'originally', 'spacewalk', 'astronaut', 'enormous', 'concern', 'mass', 'margin', 'flight', 'plan', 'eva', 'day', 'mission', 'reserve', 'eva', 'emergency', 'eva', 'obviously', 'come', 'somewhere', 'guess', 'om', 'burn', 'fuel', 'boost', 'margin', 'figure', 'goldin', 'want', 'really', 'prove', 'fast', 'cheaper', 'well', 'whiz', 'kid', 'slap', 'together', 'expendable', 'space', 'manuevere', 'tug', 'bus', 'boost', 'well', 'use', 'discovery', 'tow', 'truck', 'pat']\n"
     ]
    }
   ],
   "source": [
    "#print(data[0])\n",
    "print(data_lemmatized[9000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9acd550",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 9156/9156 [00:00<00:00, 41936.72it/s]\n"
     ]
    }
   ],
   "source": [
    "data_lemmatized_min_length = []\n",
    "\n",
    "for sublist in tqdm(data_lemmatized):\n",
    "    # Use a list comprehension to filter out strings with less than two characters\n",
    "    sublist = [word for word in sublist if len(word) > 2]\n",
    "    data_lemmatized_min_length.append(sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e008bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['archive', 'name', 'ripem', 'faq', 'last', 'update', 'sun', 'mar', 'post', 'still', 'rather', 'rough', 'list', 'likely', 'question', 'information', 'ripem', 'program', 'public', 'key', 'mail', 'encryption', 'faq', 'ripem', 'write', 'maintain', 'marc', 'vanheyningen', 'mvanheyn', 'whale', 'indiana', 'post', 'variety', 'newsgroup', 'monthly', 'basis', 'follow', 'discussion', 'specific', 'ripem', 'redirect', 'group', 'alt', 'security', 'ripem', 'month', 'reformatte', 'post', 'attempt', 'comply', 'standard', 'hypertext', 'faq', 'formatting', 'allow', 'easy', 'manipulation', 'document', 'world', 'wide', 'web', 'let', 'know', 'think', 'disclaimer', 'nothing', 'faq', 'consider', 'legal', 'advice', 'anything', 'one', 'person', 'opinion', 'want', 'real', 'legal', 'advice', 'talk', 'real', 'lawyer', 'question', 'answer', 'ripem', 'ripem', 'program', 'perform', 'privacy', 'enhance', 'mail', 'pem', 'use', 'cryptographic', 'technique', 'rsa', 'des', 'allow', 'electronic', 'mail', 'property', 'authentication', 'send', 'confirm', 'privacy', 'nobody', 'read', 'except', 'intend', 'recipient', 'ripem', 'write', 'primarily', 'mark', 'riordan', 'mrr', 'scss', 'msu', 'code', 'public', 'domain', 'except', 'rsa', 'routine', 'library', 'call', 'rsaref', 'license', 'rsa', 'datum', 'security', 'inc', 'get', 'ripem', 'ripem', 'contain', 'library', 'cryptographic', 'routine', 'rsaref', 'consider', 'munition', 'thus', 'export', 'restricted', 'distribution', 'people', 'citizen', 'permanent', 'resident', 'canada', 'therefore', 'follow', 'request', 'quote', 'readme', 'file', 'please', 'export', 'cryptographic', 'code', 'distribution', 'outside', 'usa', 'canada', 'personal', 'request', 'author', 'ripem', 'condition', 'ripem', 'note', 'rsaref', 'public', 'domain', 'license', 'include', 'distribution', 'read', 'use', 'ripem', 'good', 'way', 'get', 'ask', 'friend', 'copy', 'since', 'reduce', 'load', 'site', 'carry', 'mention', 'human', 'run', 'naturally', 'require', 'trust', 'friend', 'ripem', 'available', 'via', 'anonymous', 'ftp', 'citizen', 'permanent', 'resident', 'rsa', 'com', 'rsaref', 'read', 'readme', 'file', 'info', 'last', 'look', 'site', 'contain', 'source', 'tree', 'contain', 'compile', 'binary', 'nice', 'mac', 'version', 'ripem', 'well', 'crypt', 'stuff', 'home', 'site', 'rpub', 'msu', 'open', 'non', 'anonymous', 'ftp', 'user', 'canada', 'citizen', 'permanent', 'resident', 'find', 'obtain', 'access', 'ftp', 'pub', 'crypt', 'read', 'file', 'getting_access', 'convenience', 'binary', 'many', 'architecture', 'available', 'addition', 'full', 'source', 'tree', 'ripem', 'run', 'machine', 'probably', 'already', 'port', 'flavor', 'unix', 'sunos', 'next', 'linux', 'aix', 'ultrix', 'solaris', 'etc', 'port', 'macintosh', 'include', 'standard', 'unix', 'style', 'port', 'rather', 'nice', 'mac', 'like', 'port', 'write', 'raymond', 'lau', 'author', 'stuffit', 'port', 'expect', 'help', 'user', 'invite', 'ripem', 'work', 'mailer', 'probably', 'easy', 'clean', 'effective', 'interface', 'depend', 'sophistication', 'modularity', 'mailer', 'though', 'user', 'guide', 'include', 'distribution', 'discuss', 'way', 'ripem', 'many', 'popular', 'mailer', 'include', 'berkeley', 'mush', 'elm', 'code', 'also', 'include', 'elisp', 'allow', 'easy', 'ripem', 'inside', 'gnu', 'emac', 'make', 'new', 'interface', 'ripem', 'create', 'improvement', 'one', 'distribution', 'believe', 'convenient', 'secure', 'may', 'useful', 'other', 'feel', 'free', 'post', 'alt', 'security', 'ripem', 'rsa', 'rsa', 'crypto', 'system', 'asymmetric', 'public', 'key', 'mean', 'two', 'different', 'related', 'key', 'one', 'encrypt', 'one', 'decrypt', 'one', 'can', 'not', 'reasonably', 'derive', 'may', 'publish', 'encryption', 'public', 'key', 'widely', 'keep', 'decryption', 'private', 'key', 'anyone', 'public', 'key', 'encrypt', 'message', 'hold', 'private', 'key', 'need', 'decrypt', 'note', 'message', 'send', 'rsa', 'normally', 'des', 'key', 'real', 'message', 'see', 'des', 'note', 'provide', 'privacy', 'authentication', 'fingerprint', 'message', 'see', 'fingerprint', 'like', 'encrypted', 'sender', 'private', 'key', 'recipient', 'sender', 'public', 'key', 'decrypt', 'confirm', 'message', 'must', 'come', 'sender', 'rsa', 'name', 'three', 'man', 'rivest', 'shamir', 'adleman', 'invent', 'find', 'rsa', 'ftp', 'rsa', 'com', 'look', 'pub', 'faq', 'look', 'sci', 'crypt', 'des', 'des', 'datum', 'encryption', 'standard', 'widely', 'use', 'symmetric', 'secret', 'key', 'crypto', 'system', 'unlike', 'rsa', 'des', 'use', 'key', 'encrypt', 'decrypt', 'message', 'however', 'des', 'much', 'fast', 'rsa', 'ripem', 'use', 'des', 'rsa', 'generate', 'random', 'key', 'encrypt', 'mail', 'des', 'use', 'key', 'encrypt', 'key', 'recipient', 'public', 'rsa', 'key', 'include', 'result', 'letter', 'allow', 'recipient', 'recover', 'des', 'key', 'des', 'sometimes', 'consider', 'weak', 'somewhat', 'old', 'use', 'key', 'length', 'consider', 'short', 'modern', 'standard', 'however', 'reasonably', 'safe', 'opponent', 'small', 'large', 'corporation', 'government', 'agency', 'unlikely', 'future', 'ripem', 'strengthen', 'symmetric', 'cipher', 'possibly', 'use', 'multiple', 'encryption', 'des', 'pem', 'ripem', 'relate', 'pem', 'privacy', 'enhance', 'mail', 'system', 'allow', 'easy', 'transfer', 'encrypt', 'electronic', 'mail', 'describe', 'rfc', 'document', 'approve', 'obsolete', 'old', 'rfcs', 'ripem', 'really', 'complete', 'implementation', 'pem', 'pem', 'specify', 'certificate', 'authenticate', 'key', 'ripem', 'handle', 'time', 'addition', 'plan', 'distribute', 'authenticate', 'key', 'remote', 'user', 'able', 'send', 'secure', 'mail', 'must', 'know', 'public', 'key', 'able', 'confirm', 'message', 'receive', 'come', 'must', 'know', 'public', 'key', 'important', 'information', 'accurate', 'bad', 'guy', 'convince', 'key', 'fact', 'send', 'message', 'read', 'ripem', 'allow', 'three', 'method', 'key', 'management', 'central', 'server', 'distribute', 'finger', 'server', 'flat', 'file', 'three', 'describe', 'ripem', 'user', 'guide', 'part', 'distribution', 'none', 'provide', 'perfect', 'security', 'ripem', 'public', 'key', 'look', 'similar', 'ripem', 'public', 'key', 'begin', 'pkcs', 'identifier', 'describe', 'various', 'characteristic', 'key', 'first', 'bunch', 'character', 'key', 'may', 'lots', 'people', 'key', 'mean', 'key', 'similar', 'class', 'key', 'generate', 'program', 'length', 'etc', 'fingerprint', 'like', 'message', 'digest', 'algorithm', 'produce', 'rsa', 'datum', 'security', 'inc', 'provide', 'bit', 'fingerprint', 'secure', 'hash', 'plaintext', 'secure', 'possible', 'reasonable', 'amount', 'computation', 'produce', 'different', 'plaintext', 'produce', 'fingerprint', 'thus', 'instead', 'sign', 'entire', 'message', 'sender', 'private', 'key', 'message', 'need', 'sign', 'authentication', 'sometimes', 'use', 'purpose', 'example', 'often', 'use', 'map', 'input', 'arbitrary', 'length', 'bit', 'datum', 'passphrase', 'interpreter', 'cookie', 'generator', 'describe', 'entirety', 'include', 'implementation', 'rfc', 'pgp', 'pgp', 'another', 'cryptographic', 'mail', 'program', 'call', 'pretty', 'good', 'privacy', 'pgp', 'around', 'long', 'ripem', 'work', 'somewhat', 'differently', 'pgp', 'compatible', 'ripem', 'way', 'though', 'pgp', 'also', 'rsa', 'major', 'difference', 'pgp', 'ripem', 'pgp', 'key', 'management', 'feature', 'particularly', 'user', 'without', 'direct', 'network', 'connection', 'ripem', 'conform', 'pem', 'rfc', 'thus', 'great', 'probability', 'work', 'pem', 'software', 'pgp', 'make', 'attempt', 'compatible', 'anything', 'pgp', 'fact', 'pgp', 'compatible', 'pgp', 'ripem', 'use', 'rsaref', 'library', 'rsa', 'routine', 'rsa', 'datum', 'security', 'inc', 'rsaref', 'come', 'license', 'allow', 'noncommercial', 'pgp', 'use', 'implementation', 'rsa', 'license', 'thus', 'pkp', 'firm', 'hold', 'patent', 'rsa', 'algorithm', 'claim', 'infringement', 'patent', 'make', 'sell', 'pgp', 'canada', 'acknowledgement', 'pgp', 'original', 'author', 'phil', 'zimmermann', 'say', 'documentation', 'fact', 'live', 'usa', 'federal', 'agency', 'actually', 'run', 'pgp', 'computer', 'public', 'key', 'partner', 'want', 'forbid', 'run', 'software', 'pgp', 'contraband', 'pgp', 'ripem', 'export', 'restrict', 'can', 'not', 'send', 'outside', 'canada', 'however', 'pgp', 'already', 'exist', 'many', 'ftp', 'site', 'europe', 'place', 'whether', 'pgp', 'ripem', 'whatever', 'documentation', 'pgp', 'recommend', 'read', 'anyone', 'interested', 'issue', 'note', 'fact', 'regard', 'patent', 'export', 'restriction', 'somewhat', 'controversial', 'many', 'people', 'think', 'way', 'people', 'interpret', 'various', 'document', 'differently', 'unfortunately', 'discussion', 'net', 'inevitably', 'seem', 'produce', 'heat', 'light', 'probably', 'belong', 'misc', 'legal', 'computing', 'see', 'disclaimer', 'rpem', 'rpem', 'stand', 'rabin', 'privacy', 'enhance', 'mail', 'similar', 'ripem', 'use', 'public', 'key', 'cipher', 'invent', 'rabin', 'rsa', 'attempt', 'avoid', 'patent', 'rsa', 'write', 'mark', 'riordan', 'author', 'ripem', 'distribution', 'halt', 'contrary', 'belief', 'many', 'include', 'rabin', 'public', 'key', 'partner', 'pkp', 'claim', 'patent', 'broad', 'enough', 'cover', 'public', 'key', 'cipher', 'whose', 'strength', 'rest', 'difficulty', 'factor', 'product', 'large', 'prime', 'rsa', 'claim', 'universally', 'accept', 'mean', 'challenge', 'pragmatic', 'reason', 'rpem', 'really', 'use', 'anymore', 'compatible', 'ripem', 'pgp', 'mime', 'mime', 'stand', 'multipurpose', 'internet', 'mail', 'extension', 'describe', 'rfc', 'find', 'newsgroup', 'comp', 'mail', 'mime', 'pem', 'interact', 'mime', 'yet', 'entirely', 'clear', 'people', 'stopgap', 'solution', 'mime', 'type', 'application', 'ripem', 'order', 'send', 'ripem', 'message', 'mime', 'one', 'hope', 'standard', 'emerge', 'draft', 'internet', 'document', 'exist', 'matter', 'simple', 'way', 'defeat', 'security', 'ripem', 'may', 'wish', 'check', 'companion', 'post', 'ripem', 'attack', 'discuss', 'obvious', 'attack', 'ripem', 'security', 'procedure', 'minimize', 'risk', 'ripem', 'main', 'weak', 'area', 'probably', 'key', 'distribution']\n"
     ]
    }
   ],
   "source": [
    "#print(data[0])\n",
    "#print(data_lemmatized[0])\n",
    "print(data_lemmatized_min_length[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bca45e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 2), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 2), (7, 1), (8, 2), (9, 2), (10, 1), (11, 2), (12, 7), (13, 2), (14, 2), (15, 2), (16, 1), (17, 2), (18, 1), (19, 1), (20, 1), (21, 2), (22, 2), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 2), (33, 3), (34, 2), (35, 3), (36, 4), (37, 2), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 2), (47, 2), (48, 1), (49, 1), (50, 2), (51, 2), (52, 5), (53, 1), (54, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 3), (61, 3), (62, 3), (63, 1), (64, 1), (65, 1), (66, 3), (67, 2), (68, 3), (69, 1), (70, 1), (71, 4), (72, 1), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1), (78, 1), (79, 3), (80, 1), (81, 1), (82, 4), (83, 3), (84, 1), (85, 1), (86, 1), (87, 1), (88, 1), (89, 1), (90, 1), (91, 1), (92, 1), (93, 1), (94, 1), (95, 3), (96, 2), (97, 4), (98, 5), (99, 4), (100, 1), (101, 1), (102, 1), (103, 1), (104, 12), (105, 5), (106, 1), (107, 2), (108, 2), (109, 1), (110, 1), (111, 1), (112, 2), (113, 2), (114, 2), (115, 2), (116, 8), (117, 4), (118, 2), (119, 2), (120, 1), (121, 4), (122, 1), (123, 2), (124, 1), (125, 1), (126, 1), (127, 1), (128, 6), (129, 1), (130, 4), (131, 3), (132, 1), (133, 1), (134, 1), (135, 1), (136, 2), (137, 1), (138, 1), (139, 2), (140, 2), (141, 1), (142, 4), (143, 1), (144, 4), (145, 1), (146, 5), (147, 1), (148, 1), (149, 1), (150, 1), (151, 4), (152, 3), (153, 1), (154, 5), (155, 1), (156, 1), (157, 1), (158, 1), (159, 2), (160, 1), (161, 1), (162, 1), (163, 2), (164, 5), (165, 1), (166, 1), (167, 2), (168, 1), (169, 2), (170, 1), (171, 1), (172, 2), (173, 1), (174, 1), (175, 1), (176, 2), (177, 1), (178, 1), (179, 1), (180, 1), (181, 1), (182, 1), (183, 2), (184, 1), (185, 1), (186, 3), (187, 1), (188, 1), (189, 1), (190, 3), (191, 1), (192, 1), (193, 3), (194, 8), (195, 1), (196, 1), (197, 1), (198, 2), (199, 1), (200, 1), (201, 1), (202, 1), (203, 1), (204, 1), (205, 1), (206, 2), (207, 2), (208, 1), (209, 1), (210, 2), (211, 1), (212, 1), (213, 1), (214, 38), (215, 3), (216, 2), (217, 2), (218, 1), (219, 1), (220, 3), (221, 3), (222, 1), (223, 1), (224, 3), (225, 4), (226, 1), (227, 3), (228, 1), (229, 1), (230, 1), (231, 1), (232, 1), (233, 1), (234, 4), (235, 1), (236, 2), (237, 1), (238, 1), (239, 11), (240, 3), (241, 1), (242, 1), (243, 1), (244, 3), (245, 1), (246, 2), (247, 1), (248, 5), (249, 1), (250, 1), (251, 1), (252, 2), (253, 1), (254, 4), (255, 3), (256, 1), (257, 12), (258, 1), (259, 6), (260, 1), (261, 1), (262, 1), (263, 1), (264, 1), (265, 1), (266, 1), (267, 2), (268, 1), (269, 1), (270, 1), (271, 1), (272, 1), (273, 3), (274, 1), (275, 2), (276, 1), (277, 2), (278, 1), (279, 1), (280, 1), (281, 2), (282, 1), (283, 2), (284, 1), (285, 1), (286, 1), (287, 1), (288, 1), (289, 2), (290, 4), (291, 1), (292, 1), (293, 1), (294, 1), (295, 1), (296, 2), (297, 6), (298, 1), (299, 1), (300, 1), (301, 1), (302, 1), (303, 1), (304, 2), (305, 1), (306, 1), (307, 2), (308, 1), (309, 5), (310, 8), (311, 5), (312, 1), (313, 1), (314, 3), (315, 1), (316, 1), (317, 21), (318, 1), (319, 1), (320, 2), (321, 1), (322, 2), (323, 1), (324, 1), (325, 1), (326, 5), (327, 1), (328, 1), (329, 5), (330, 1), (331, 1), (332, 1), (333, 1), (334, 6), (335, 4), (336, 1), (337, 4), (338, 1), (339, 4), (340, 1), (341, 4), (342, 1), (343, 3), (344, 2), (345, 16), (346, 1), (347, 1), (348, 2), (349, 1), (350, 3), (351, 1), (352, 2), (353, 1), (354, 6), (355, 2), (356, 3), (357, 2), (358, 1), (359, 1), (360, 2), (361, 1), (362, 4), (363, 1), (364, 1), (365, 1), (366, 1), (367, 1), (368, 1), (369, 1), (370, 1), (371, 1), (372, 2), (373, 1), (374, 3), (375, 1), (376, 1), (377, 1), (378, 1), (379, 1), (380, 4), (381, 1), (382, 2), (383, 46), (384, 1), (385, 1), (386, 1), (387, 3), (388, 3), (389, 1), (390, 23), (391, 6), (392, 4), (393, 1), (394, 1), (395, 1), (396, 1), (397, 1), (398, 4), (399, 8), (400, 3), (401, 1), (402, 1), (403, 6), (404, 4), (405, 2), (406, 1), (407, 1), (408, 2), (409, 3), (410, 1), (411, 1), (412, 4), (413, 1), (414, 2), (415, 1), (416, 1), (417, 2), (418, 3), (419, 1), (420, 2), (421, 1), (422, 1), (423, 2), (424, 5), (425, 1), (426, 1), (427, 1), (428, 1), (429, 1), (430, 1), (431, 1), (432, 1), (433, 1), (434, 2), (435, 3), (436, 1), (437, 1), (438, 1), (439, 2), (440, 2), (441, 3), (442, 4), (443, 1), (444, 1), (445, 2), (446, 1), (447, 1), (448, 1), (449, 1), (450, 1), (451, 1), (452, 2), (453, 1), (454, 1), (455, 1), (456, 2), (457, 14), (458, 1), (459, 6), (460, 1), (461, 1), (462, 2), (463, 1), (464, 1), (465, 2), (466, 5), (467, 2), (468, 1), (469, 1), (470, 1), (471, 1), (472, 1), (473, 1), (474, 1), (475, 2), (476, 1), (477, 1), (478, 3), (479, 1), (480, 4), (481, 1), (482, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized_min_length)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized_min_length\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View \n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75bd1069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9156\n",
      "48318\n"
     ]
    }
   ],
   "source": [
    "# Number of documents\n",
    "print(len(corpus))\n",
    "\n",
    "# Size of the vocabulary\n",
    "print(len(id2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89658261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101.0\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "def sum_of_second_components(tuple_list):\n",
    "    total_sum = 0\n",
    "    for tup in tuple_list:\n",
    "        total_sum += tup[1]  # Accessing the second component of each tuple\n",
    "    return total_sum\n",
    "\n",
    "lengths = []\n",
    "for doc in corpus:\n",
    "    lengths.append(sum_of_second_components(doc))\n",
    "print(statistics.median(lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de9f1c8",
   "metadata": {},
   "source": [
    "### Topic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daacb8c",
   "metadata": {},
   "source": [
    "#### Vector Space Model (VSM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36ab8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import dok_matrix\n",
    "\n",
    "# Define function to convert Gensim corpus to a sparse pandas DataFrame\n",
    "def corpus_to_sparse_dataframe(corpus):\n",
    "    word_freq = dok_matrix((len(corpus), len(id2word)), dtype=int)\n",
    "\n",
    "    for i, doc in enumerate(corpus):\n",
    "        for word_id, freq in doc:\n",
    "            word_freq[i, word_id] = freq\n",
    "\n",
    "    dataframe = pd.DataFrame.sparse.from_spmatrix(word_freq)\n",
    "    dataframe.columns = [id2word[word_id] for word_id in range(len(id2word))]\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1e9087",
   "metadata": {},
   "outputs": [],
   "source": [
    "VSM = corpus_to_sparse_dataframe(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f80644",
   "metadata": {},
   "source": [
    "#### Vector Space Model and tf-idf (VSM & tf-idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fe0f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel\n",
    "\n",
    "model = TfidfModel(corpus)  # fit model\n",
    "tfidf_corpus = model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f79a2de",
   "metadata": {},
   "source": [
    "#### Latent Semantic Indexing (LSI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6412e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LsiModel\n",
    "\n",
    "K = 8\n",
    "lsi_model = LsiModel(corpus, id2word=id2word, num_topics=K)\n",
    "lsi_model.print_topics(num_topics=K, num_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7427d7c",
   "metadata": {},
   "source": [
    "#### Latent Semantic Indexing and tf-idf (LSI & tf-idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfed574",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LsiModel\n",
    "\n",
    "K = 8\n",
    "tfidf_lsi_model = LsiModel(tfidf_corpus, id2word=id2word, num_topics=K)\n",
    "tfidf_lsi_model.print_topics(num_topics=K, num_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece98c7c",
   "metadata": {},
   "source": [
    "#### Non-Negative Matrix Factorization (NMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b3be5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/nmf_tutorial.ipynb\n",
    "from gensim.models.nmf import Nmf\n",
    "\n",
    "K = 8\n",
    "nmf_model = Nmf(corpus, id2word=id2word, num_topics=K)\n",
    "nmf_model.show_topics(num_topics=K, num_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208a80fd",
   "metadata": {},
   "source": [
    "#### Non-Negative Matrix Factorization and tf-idf (NMF & tf-idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceb2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/nmf_tutorial.ipynb\n",
    "from gensim.models.nmf import Nmf\n",
    "\n",
    "K = 8\n",
    "tfidf_nmf_model = Nmf(tfidf_corpus, id2word=id2word, num_topics=K)\n",
    "tfidf_nmf_model.show_topics(num_topics=K, num_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09d7da5",
   "metadata": {},
   "source": [
    "#### Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e721a915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "K = 8\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=K,\n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=400,\n",
    "                                           passes=30,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "# Print the Keywords in the 3 topics\n",
    "pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017b7803",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
