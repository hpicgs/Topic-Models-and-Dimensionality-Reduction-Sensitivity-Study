{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ff6e030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/datasets/kristianperriu/genereclassification\n",
    "# https://www.kaggle.com/datasets/karnikakapoor/lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "095bc864",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel Atzberger\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.4)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n",
      "C:\\Users\\Daniel Atzberger\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "import spacy\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from','subject','re','edu','use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe832530",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/Lyrics/lyrics_dataframe_clean_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c26182dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>genere</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>\\nyo i never fucked wayne i never fucked drake...</td>\n",
       "      <td>rap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>\\nyo b they ready\\nlet's go\\n\\nfeelin' myself ...</td>\n",
       "      <td>rap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>\\nuh mmm kyuh\\nrip to big \\nclassic shit\\n\\ni'...</td>\n",
       "      <td>rap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\\nyou know yeah\\ntouchin' yeah\\nnight of\\nyou ...</td>\n",
       "      <td>rap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>\\nayo look like i'm goin' for a swim\\ndunked o...</td>\n",
       "      <td>rap</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                             lyrics genere\n",
       "0           0  \\nyo i never fucked wayne i never fucked drake...    rap\n",
       "1           1  \\nyo b they ready\\nlet's go\\n\\nfeelin' myself ...    rap\n",
       "2           2  \\nuh mmm kyuh\\nrip to big \\nclassic shit\\n\\ni'...    rap\n",
       "3           3  \\nyou know yeah\\ntouchin' yeah\\nnight of\\nyou ...    rap\n",
       "4           4  \\nayo look like i'm goin' for a swim\\ndunked o...    rap"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08ddda38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c6a4212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "311778e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6203a711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start removing stop words\n",
      "Installing spacy\n",
      "Start lemmatizing words\n"
     ]
    }
   ],
   "source": [
    "data = df[\"lyrics\"].tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "# Remove Stop Words\n",
    "print(\"Start removing stop words\")\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# conda install -c conda-forge spacy-model-en_core_web_sm\n",
    "print(\"Installing spacy\")\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "print(\"Start lemmatizing words\")\n",
    "#data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "data_lemmatized = lemmatization(data_words_nostops, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ce114b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lemmatized_min_length = []\n",
    "\n",
    "for sublist in data_lemmatized:\n",
    "    # Use a list comprehension to filter out strings with less than two characters\n",
    "    sublist = [word for word in sublist if len(word) > 2]\n",
    "    data_lemmatized_min_length.append(sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "950a460c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tay', 'keith', 'fuck', 'niggas', 'type', 'guy', 'like', 'since', 'middle', 'school', 'never', 'think', 'get', 'famous', 'always', 'know', 'cool', 'could', 'fuck', 'shawty', 'wanna', 'rude', 'eliantte', 'diamond', 'want', 'fine', 'jewel', 'hitter', 'dem', 'slidin', 'put', 'camera', 'crew', 'doin', 'bad', 'somebody', 'google', 'never', 'worry', 'bout', 'say', 'gon', 'put', 'overtime', 'doin', 'number', 'like', 'two', 'back', 'time', 'change', 'new', 'money', 'conversation', 'table', 'speak', 'fluently', 'stay', 'away', 'bad', 'vibe', 'see', 'nigga', 'still', 'give', 'couple', 'thousand', 'even', 'though', 'would', 'free', 'treat', 'like', 'rapper', 'cause', 'vibe', 'like', 'street', 'one', 'top', 'five', 'many', 'guy', 'real', 'barely', 'get', 'sleep', 'tryna', 'make', 'sure', 'everybody', 'eat', 'take', 'trip', 'overseas', 'buyin', 'drip', 'currency', 'fire', 'right', 'somethin', 'currently', 'bitch', 'tell', 'lie', 'tell', 'curvin', 'big', 'dawgs', 'gon', 'let', 'people', 'work', 'get', 'problem', 'emergency', 'yeah', 'hear', 'crime', 'pay', 'think', 'thug', 'cause', 'get', 'ice', 'plain', 'jane', 'matter', 'least', 'quarter', 'main', 'thing', 'show', 'forbe', 'complain', 'tryna', 'maintain', 'hop', 'plane', 'pick', 'check', 'thing', 'hear', 'crime', 'pay', 'think', 'thug', 'cause', 'get', 'ice', 'plain', 'jane', 'matter', 'least', 'quarter', 'main', 'thing', 'show', 'forbe', 'complain', 'tryna', 'maintain', 'hop', 'plane', 'pick', 'check', 'thing', 'see', 'lil', 'baby', 'liveget', 'ticket', 'low', 'might', 'also', 'like', 'lately', 'diggin', 'vibe', 'tell', 'look', 'eye', 'need', 'get', 'shit', 'mind', 'give', 'time', 'give', 'jet', 'sky', 'runnin', 'round', 'lap', 'perform', 'clap', 'get', 'problem', 'head', 'chat', 'runnin', 'around', 'globe', 'chasing', 'rack', 'plastic', 'sometimes', 'sarcastic', 'like', 'hear', 'laughter', 'tryna', 'ease', 'pain', 'take', 'fast', 'know', 'hate', 'pass', 'get', 'grave', 'throwin', 'dirt', 'name', 'yeah', 'hear', 'crime', 'pay', 'think', 'thug', 'cause', 'get', 'ice', 'plain', 'jane', 'matter', 'least', 'quarter', 'main', 'thing', 'show', 'forbe', 'complain', 'tryna', 'maintain', 'hop', 'plane', 'pick', 'check', 'thing', 'hear', 'crime', 'pay', 'think', 'thug', 'cause', 'get', 'ice', 'plain', 'jane', 'matter', 'least', 'quarter', 'main', 'thing', 'show', 'forbe', 'complain', 'tryna', 'maintain', 'hop', 'plane', 'pick', 'check', 'thing']\n"
     ]
    }
   ],
   "source": [
    "print(data_lemmatized_min_length[348])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3966b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 3), (4, 1), (5, 1), (6, 1), (7, 3), (8, 1), (9, 1), (10, 8), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 3), (19, 18), (20, 1), (21, 2), (22, 3), (23, 3), (24, 4), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 3), (32, 1), (33, 3), (34, 1), (35, 1), (36, 1), (37, 1), (38, 2), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 3), (45, 1), (46, 1), (47, 2), (48, 2), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 1), (56, 1), (57, 3), (58, 1), (59, 1), (60, 1), (61, 1), (62, 3), (63, 1), (64, 1), (65, 2), (66, 1), (67, 1), (68, 1), (69, 3), (70, 1), (71, 19), (72, 1), (73, 1), (74, 8), (75, 2), (76, 1), (77, 2), (78, 1), (79, 1), (80, 1), (81, 1), (82, 2), (83, 1), (84, 1), (85, 1), (86, 1), (87, 2), (88, 2), (89, 1), (90, 1), (91, 1), (92, 1), (93, 4), (94, 1), (95, 1), (96, 3), (97, 1), (98, 1), (99, 1), (100, 3), (101, 1), (102, 2), (103, 5), (104, 3), (105, 1), (106, 1), (107, 1), (108, 2), (109, 2), (110, 14), (111, 1), (112, 1), (113, 1), (114, 1), (115, 1), (116, 1), (117, 1), (118, 5), (119, 1), (120, 1), (121, 1), (122, 1), (123, 4), (124, 1), (125, 1), (126, 1), (127, 3), (128, 1), (129, 1), (130, 1), (131, 1), (132, 2), (133, 1), (134, 1), (135, 4), (136, 1), (137, 2), (138, 12), (139, 9), (140, 1), (141, 6), (142, 1), (143, 2), (144, 1), (145, 1), (146, 1), (147, 1), (148, 1), (149, 1), (150, 1), (151, 1), (152, 1), (153, 1), (154, 4), (155, 1), (156, 1), (157, 3), (158, 1), (159, 1), (160, 3), (161, 1), (162, 8), (163, 1), (164, 1), (165, 3), (166, 2), (167, 1), (168, 1), (169, 3), (170, 1), (171, 1), (172, 2), (173, 1), (174, 1), (175, 1), (176, 1), (177, 1), (178, 1), (179, 2), (180, 1), (181, 3), (182, 1), (183, 1), (184, 1), (185, 3), (186, 1), (187, 1), (188, 1), (189, 1), (190, 3), (191, 3), (192, 1), (193, 1), (194, 1), (195, 1), (196, 1), (197, 3), (198, 2), (199, 1), (200, 1), (201, 2), (202, 1), (203, 5), (204, 1), (205, 1), (206, 2), (207, 2), (208, 1), (209, 1), (210, 1), (211, 5), (212, 1), (213, 1), (214, 1), (215, 1), (216, 1), (217, 2), (218, 3), (219, 1), (220, 1), (221, 1), (222, 1), (223, 1), (224, 2), (225, 1), (226, 1), (227, 1), (228, 1), (229, 1), (230, 1), (231, 2), (232, 1), (233, 1), (234, 1), (235, 1), (236, 1), (237, 1), (238, 2), (239, 7), (240, 1), (241, 1), (242, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized_min_length)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized_min_length\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View \n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67dda84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10995\n",
      "32758\n"
     ]
    }
   ],
   "source": [
    "# Number of documents\n",
    "print(len(corpus))\n",
    "\n",
    "# Size of the vocabulary\n",
    "print(len(id2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb5ba847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "def sum_of_second_components(tuple_list):\n",
    "    total_sum = 0\n",
    "    for tup in tuple_list:\n",
    "        total_sum += tup[1]  # Accessing the second component of each tuple\n",
    "    return total_sum\n",
    "\n",
    "lengths = []\n",
    "for doc in corpus:\n",
    "    lengths.append(sum_of_second_components(doc))\n",
    "print(statistics.median(lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09564f8b",
   "metadata": {},
   "source": [
    "### Topic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a32276f",
   "metadata": {},
   "source": [
    "#### Vector Space Model (VSM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98425a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import dok_matrix\n",
    "\n",
    "# Define function to convert Gensim corpus to a sparse pandas DataFrame\n",
    "def corpus_to_sparse_dataframe(corpus):\n",
    "    word_freq = dok_matrix((len(corpus), len(id2word)), dtype=int)\n",
    "\n",
    "    for i, doc in enumerate(corpus):\n",
    "        for word_id, freq in doc:\n",
    "            word_freq[i, word_id] = freq\n",
    "\n",
    "    dataframe = pd.DataFrame.sparse.from_spmatrix(word_freq)\n",
    "    dataframe.columns = [id2word[word_id] for word_id in range(len(id2word))]\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ec1ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "VSM = corpus_to_sparse_dataframe(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56696607",
   "metadata": {},
   "source": [
    "#### Vector Space Model & tf-idf (VSM & tf-idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e70588",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel\n",
    "\n",
    "model = TfidfModel(corpus)  # fit model\n",
    "tfidf_corpus = model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91de00b0",
   "metadata": {},
   "source": [
    "#### Latent Semantic Indexing (LSI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ffb226",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LsiModel\n",
    "\n",
    "K = 12\n",
    "lsi_model = LsiModel(corpus, id2word=id2word, num_topics=K)\n",
    "lsi_model.print_topics(num_topics=K, num_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7748e6b5",
   "metadata": {},
   "source": [
    "#### Latent Semantic Indexing & tf-idf (LSI & tf-idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc325d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LsiModel\n",
    "\n",
    "K = 12\n",
    "tfidf_lsi_model = LsiModel(tfidf_corpus, id2word=id2word, num_topics=K)\n",
    "tfidf_lsi_model.print_topics(num_topics=K, num_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6750ec65",
   "metadata": {},
   "source": [
    "#### Non-Negative Matrix Factorization (NMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc310c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/nmf_tutorial.ipynb\n",
    "from gensim.models.nmf import Nmf\n",
    "\n",
    "K = 12\n",
    "nmf_model = Nmf(corpus, id2word=id2word, num_topics=K)\n",
    "nmf_model.show_topics(num_topics=K, num_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca8cd75",
   "metadata": {},
   "source": [
    "#### Non-Negative Matrix Factorization & tf-idf (NMF & tf-idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d06565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/nmf_tutorial.ipynb\n",
    "from gensim.models.nmf import Nmf\n",
    "\n",
    "K = 12\n",
    "tfidf_nmf_model = Nmf(tfidf_corpus, id2word=id2word, num_topics=K)\n",
    "tfidf_nmf_model.show_topics(num_topics=K, num_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab02d90",
   "metadata": {},
   "source": [
    "#### Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d531975a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "K = 12\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=K,\n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=400,\n",
    "                                           passes=30,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "# Print the Keywords in the 3 topics\n",
    "pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b13354e",
   "metadata": {},
   "source": [
    "### Miscallenous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33210a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "num_topics = 12\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=num_topics,\n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=400,\n",
    "                                           passes=30,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "# Print the Keywords in the 3 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f73977",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for doc in corpus:\n",
    "    doc_top = []\n",
    "    for t in lda_model.get_document_topics(doc, minimum_probability = 0):\n",
    "        doc_top.append(t[1])\n",
    "    rows.append(doc_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b695c64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_topic_matrix = pd.DataFrame(rows)\n",
    "#document_topic_matrix_sourcecode[\"identifier\"] = df_sourcecode.iloc[:,0].tolist()\n",
    "document_topic_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640d4f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "DTM = document_topic_matrix\n",
    "Y = df[\"genere\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f939d3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_int_mapping(strings):\n",
    "    # Create a dictionary to store string to integer mapping\n",
    "    string_to_int = {}\n",
    "    int_sequence = []\n",
    "\n",
    "    # Assign a unique integer to each unique string\n",
    "    for string in strings:\n",
    "        if string not in string_to_int:\n",
    "            string_to_int[string] = len(string_to_int)  # Assign the next available integer\n",
    "        # Append the corresponding integer to the sequence\n",
    "        int_sequence.append(string_to_int[string])\n",
    "\n",
    "    return int_sequence, string_to_int\n",
    "\n",
    "# Example usage:\n",
    "strings = [\"apple\", \"banana\", \"apple\", \"orange\", \"banana\"]\n",
    "int_sequence, string_to_int = string_to_int_mapping(strings)\n",
    "print(\"String to integer sequence:\", int_sequence)\n",
    "print(\"String to integer mapping:\", string_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e09a6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_converted,_ = string_to_int_mapping(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02821fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality reduction 1: tSNE\n",
    "\n",
    "import time\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "#n_sne = 7000\n",
    "\n",
    "time_start = time.time()\n",
    "#tsne = TSNE(n_iter=300)\n",
    "#tsne = TSNE(n_components=2, n_iter = 2000, perplexity=40)\n",
    "tsne = TSNE(n_components=2)\n",
    "tsne_results = tsne.fit_transform(DTM)\n",
    "\n",
    "print ('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Create the figure\n",
    "fig = plt.figure( figsize=(8,8) )\n",
    "ax = fig.add_subplot(1, 1, 1, title='TSNE' )\n",
    "# Create the scatter\n",
    "ax.scatter(\n",
    "    x=tsne_results[:,0], \n",
    "    y=tsne_results[:,1], \n",
    "    c=Y_converted, \n",
    "    cmap=plt.cm.get_cmap('Paired'), \n",
    "    alpha=1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b65a7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
