{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "108da12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/datasets/deepak711/4-subject-data-text-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "365e03b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel Atzberger\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.4)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n",
      "C:\\Users\\Daniel Atzberger\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "import spacy\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from','subject','re','edu','use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b2355ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_txt_files(directory):\n",
    "    txt_files = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                txt_files.append(os.path.join(root, file))\n",
    "    return txt_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "911e83b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding=\"utf8\") as file:\n",
    "            file_contents = file.read()\n",
    "        return file_contents\n",
    "    except FileNotFoundError:\n",
    "        return \"File not found.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3313be8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in tqdm(sentences):\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adc2d108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09561046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in tqdm(texts):\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d47620b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# category 1: Computer Science\n",
    "directory_path = \"C:\\\\Users\\\\Daniel Atzberger\\\\Documents\\\\IEEE_Vis24\\\\data\\\\Seven Categories\\\\Computer_Science\"\n",
    "file_list = find_txt_files(directory_path)\n",
    "\n",
    "file_contents_ComputerScience = []\n",
    "labels_ComputerScience = []\n",
    "for file in file_list:\n",
    "    file_contents_ComputerScience.append(read_txt_file(file))\n",
    "    labels_ComputerScience.append(\"Computer Science\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff61dde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# category 2: History\n",
    "directory_path = \"C:\\\\Users\\\\Daniel Atzberger\\\\Documents\\\\IEEE_Vis24\\\\data\\\\Seven Categories\\\\History\"\n",
    "file_list = find_txt_files(directory_path)\n",
    "\n",
    "file_contents_History = []\n",
    "labels_History = []\n",
    "for file in file_list:\n",
    "    file_contents_History.append(read_txt_file(file))\n",
    "    labels_History.append(\"History\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92b09ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# category 3: Maths\n",
    "directory_path = \"C:\\\\Users\\\\Daniel Atzberger\\\\Documents\\\\IEEE_Vis24\\\\data\\\\Seven Categories\\\\Maths\"\n",
    "file_list = find_txt_files(directory_path)\n",
    "\n",
    "file_contents_Maths = []\n",
    "labels_Maths = []\n",
    "for file in file_list:\n",
    "    file_contents_Maths.append(read_txt_file(file))\n",
    "    labels_Maths.append(\"Maths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51134058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# category 4: accounts\n",
    "directory_path = \"C:\\\\Users\\\\Daniel Atzberger\\\\Documents\\\\IEEE_Vis24\\\\data\\\\Seven Categories\\\\Physics_Biology_Geography_Accounts subject training data for text classification\\\\train_data_final\\\\accounts\"\n",
    "file_list = find_txt_files(directory_path)\n",
    "\n",
    "file_contents_accounts = []\n",
    "labels_accounts = []\n",
    "for file in file_list:\n",
    "    file_contents_accounts.append(read_txt_file(file))\n",
    "    labels_accounts.append(\"accounts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "191a0161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# category 5: physics\n",
    "directory_path = \"C:\\\\Users\\\\Daniel Atzberger\\\\Documents\\\\IEEE_Vis24\\\\data\\\\Seven Categories\\\\Physics_Biology_Geography_Accounts subject training data for text classification\\\\train_data_final\\\\physics\"\n",
    "file_list = find_txt_files(directory_path)\n",
    "\n",
    "file_contents_physics = []\n",
    "labels_physics = []\n",
    "for file in file_list:\n",
    "    file_contents_physics.append(read_txt_file(file))\n",
    "    labels_physics.append(\"physics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ea973d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# category 6: geography\n",
    "directory_path = \"C:\\\\Users\\\\Daniel Atzberger\\\\Documents\\\\IEEE_Vis24\\\\data\\\\Seven Categories\\\\Physics_Biology_Geography_Accounts subject training data for text classification\\\\train_data_final\\\\geography\"\n",
    "file_list = find_txt_files(directory_path)\n",
    "\n",
    "file_contents_geography = []\n",
    "labels_geography = []\n",
    "for file in file_list:\n",
    "    file_contents_geography.append(read_txt_file(file))\n",
    "    labels_geography.append(\"geography\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1be2bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# category 7: biology\n",
    "directory_path = \"C:\\\\Users\\\\Daniel Atzberger\\\\Documents\\\\IEEE_Vis24\\\\data\\\\Seven Categories\\\\Physics_Biology_Geography_Accounts subject training data for text classification\\\\train_data_final\\\\biology\"\n",
    "file_list = find_txt_files(directory_path)\n",
    "\n",
    "file_contents_biology = []\n",
    "labels_biology = []\n",
    "for file in file_list:\n",
    "    file_contents_biology.append(read_txt_file(file))\n",
    "    labels_biology.append(\"biology\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a88db733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homework Helpers: Basic Math and Pre-Algebra\n",
      "100\n",
      "To compare two decimal numbers, start by comparing their integral\n",
      "parts. If the two decimal numbers have the same integral part, start com-\n",
      "paring their fractional parts. To compare the fractional parts, start com-\n",
      "paring the digits immediately to the right of the decimal point and keep\n",
      "going right until one number wins.\n",
      "Example 1\n",
      "Compare: 10.56 and 8.37\n",
      "Solution: Compare the integral parts of the two numbers: The\n",
      "integral part of 10.56 is 10, and the integral part of 8.37 is 8. Clearly\n",
      "10.56 wins:\n",
      "10.56> 8.37\n",
      "Example 2\n",
      "Compare: 1.369 and 1.375\n",
      "Solution: Both numbers have the same integral part, so they tie,\n",
      "and we will need to compare the fractional parts. Start with the\n",
      "tenths’ place: Both numbers have a 3 in the tenths’ place, so they\n",
      "still tie. 1.369 has a 6 in the hundredths’ place and 1.375 has a 7 in\n",
      "the hundredths’ place, so 1.375 wins:\n",
      "1.369< 1.375\n",
      "Example 3\n",
      "Compare: 1.25 and 1.254\n",
      "Solution: Both numbers have the same integral part, so they tie,\n",
      "and we will need to compare the fractional parts. Start with the\n",
      "tenths’ place: Both numbers have a 2 in the tenths’ place, so they\n",
      "still tie. Both numbers have a 5 in the hundredths’ place, so they\n",
      "continue to tie. Now compare the digits in the thousandths’ place:\n",
      "1.25 has a 0 in the thousandths’ place and 1.254 has a 4 in the\n",
      "thousandths’ place, so 1.254 wins:\n",
      "1.25< 1.254\n",
      "www.ebook3000.com\n"
     ]
    }
   ],
   "source": [
    "print(file_contents_Maths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd098e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 3142/3142 [00:03<00:00, 906.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start removing stop words\n",
      "Installing spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                | 5/3142 [00:00<01:12, 43.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start lemmatizing words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3142/3142 [00:41<00:00, 76.08it/s]\n"
     ]
    }
   ],
   "source": [
    "data = file_contents_ComputerScience + file_contents_History + file_contents_Maths + file_contents_accounts + file_contents_physics + file_contents_geography + file_contents_biology\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "# Remove Stop Words\n",
    "print(\"Start removing stop words\")\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# conda install -c conda-forge spacy-model-en_core_web_sm\n",
    "print(\"Installing spacy\")\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "print(\"Start lemmatizing words\")\n",
    "#data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "data_lemmatized = lemmatization(data_words_nostops, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4721d010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84 Chapter 2 Instructions: Language of the Computer\n",
      "The parameter variable n corresponds to the argument register $aO. The\n",
      "ANSWER\n",
      "compiled program starts with the label of the procedure and then saves two\n",
      "registers on the stack, the return address and $a0:\n",
      "fact :\n",
      "addi $sp , $sp , - 8 # adjust stack for 2 items\n",
      "sw $ra , 4($sp) # save the return address\n",
      "sw $aO , O($sp) # save the argument n\n",
      "The first time fact is called, sw saves an address in the program that called\n",
      "f act. The next two instructions test if n is less than 1, going to Ll if n ;::.: 1.\n",
      "<\n",
      "slti HO,$aO,1 It test for n 1\n",
      "beq HO,$zero,Ll It if n ) = 1 , go to Ll\n",
      "If n is less than 1, fact returns 1 by putting 1 into a value register: it adds 1\n",
      "to 0 and places that sum in $vO. It then pops the two saved values off the\n",
      "stack and jumps to the return address:\n",
      "addi hO , $zero , 1 It return 1\n",
      "addi $sp , $sp , 8 It pop 2 items off stack\n",
      "Jr Ira It return to after jal\n",
      "°\n",
      "Before popping two items off the stack, we could have loaded $a and $ra. Since\n",
      "°\n",
      "$a and $ra don't change when n is less than 1, we skip those instructions.\n",
      "If n is not less than 1, the argument n is decremented and then fact is\n",
      "called again with the decremented value:\n",
      "Ll : addi$aO , $aO , - 1 It n ) = 1 : argument gets (n - 1)\n",
      "jalfact It call fact with (n - 1)\n",
      "The next instruction is where fact returns. Now the old return address and\n",
      "old argument are restored, along with the stack pointer:\n",
      "1w $aO, O($sp) It return from jal : restore argument n\n",
      "1w $ra, 4($sp) It restore the return address\n",
      "addi $sp, $sp ,8 It adjust stack pointer to pop 2 items\n",
      "['chapter', 'instruction', 'language', 'computer', 'parameter', 'variable', 'correspond', 'argument', 'register', 'ao', 'answer', 'compile', 'program', 'start', 'label', 'procedure', 'save', 'two', 'register', 'stack', 'return', 'address', 'fact', 'addi', 'sp', 'sp', 'adjust', 'stack', 'item', 'sw', 'ra', 'sp', 'save', 'return', 'address', 'sw', 'ao', 'sp', 'save', 'argument', 'first', 'time', 'fact', 'call', 'sw', 'save', 'address', 'program', 'call', 'act', 'next', 'two', 'instruction', 'test', 'less', 'go', 'slti', 'ho', 'ao', 'test', 'beq', 'ho', 'zero', 'go', 'less', 'fact', 'return', 'put', 'value', 'register', 'add', 'place', 'sum', 'vo', 'pop', 'two', 'save', 'value', 'stack', 'jump', 'return', 'address', 'addi', 'ho', 'zero', 'return', 'addi', 'sp', 'sp', 'pop', 'item', 'stack', 'jr', 'ira', 'return', 'jal', 'pop', 'two', 'item', 'stack', 'could', 'load', 'ra', 'since', 'ra', 'change', 'less', 'skip', 'instruction', 'less', 'argument', 'decremente', 'fact', 'call', 'decremente', 'value', 'addi', 'ao', 'ao', 'argument', 'get', 'jalfact', 'call', 'fact', 'next', 'instruction', 'fact', 'return', 'old', 'return', 'address', 'old', 'argument', 'restore', 'along', 'stack', 'pointer', 'ao', 'sp', 'return', 'jal', 'restore', 'argument', 'ra', 'sp', 'restore', 'return', 'address', 'addi', 'sp', 'sp', 'adjust', 'stack', 'pointer', 'pop', 'item']\n"
     ]
    }
   ],
   "source": [
    "print(data[0])\n",
    "print(data_lemmatized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9acd550",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 3142/3142 [00:00<00:00, 36382.49it/s]\n"
     ]
    }
   ],
   "source": [
    "data_lemmatized_min_length = []\n",
    "\n",
    "for sublist in tqdm(data_lemmatized):\n",
    "    # Use a list comprehension to filter out strings with less than two characters\n",
    "    sublist = [word for word in sublist if len(word) > 2]\n",
    "    data_lemmatized_min_length.append(sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e008bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chapter', 'instruction', 'language', 'computer', 'parameter', 'variable', 'correspond', 'argument', 'register', 'answer', 'compile', 'program', 'start', 'label', 'procedure', 'save', 'two', 'register', 'stack', 'return', 'address', 'fact', 'addi', 'adjust', 'stack', 'item', 'save', 'return', 'address', 'save', 'argument', 'first', 'time', 'fact', 'call', 'save', 'address', 'program', 'call', 'act', 'next', 'two', 'instruction', 'test', 'less', 'slti', 'test', 'beq', 'zero', 'less', 'fact', 'return', 'put', 'value', 'register', 'add', 'place', 'sum', 'pop', 'two', 'save', 'value', 'stack', 'jump', 'return', 'address', 'addi', 'zero', 'return', 'addi', 'pop', 'item', 'stack', 'ira', 'return', 'jal', 'pop', 'two', 'item', 'stack', 'could', 'load', 'since', 'change', 'less', 'skip', 'instruction', 'less', 'argument', 'decremente', 'fact', 'call', 'decremente', 'value', 'addi', 'argument', 'get', 'jalfact', 'call', 'fact', 'next', 'instruction', 'fact', 'return', 'old', 'return', 'address', 'old', 'argument', 'restore', 'along', 'stack', 'pointer', 'return', 'jal', 'restore', 'argument', 'restore', 'return', 'address', 'addi', 'adjust', 'stack', 'pointer', 'pop', 'item']\n"
     ]
    }
   ],
   "source": [
    "#print(data[0])\n",
    "#print(data_lemmatized[0])\n",
    "print(data_lemmatized_min_length[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54fc53e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 5), (3, 6), (4, 2), (5, 1), (6, 1), (7, 6), (8, 1), (9, 4), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 2), (17, 6), (18, 1), (19, 1), (20, 4), (21, 1), (22, 4), (23, 2), (24, 1), (25, 1), (26, 1), (27, 1), (28, 4), (29, 1), (30, 2), (31, 2), (32, 1), (33, 1), (34, 2), (35, 4), (36, 1), (37, 2), (38, 1), (39, 3), (40, 3), (41, 10), (42, 5), (43, 1), (44, 1), (45, 1), (46, 7), (47, 1), (48, 1), (49, 2), (50, 1), (51, 4), (52, 3), (53, 1), (54, 2)]]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized_min_length)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized_min_length\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View \n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017aaace",
   "metadata": {},
   "source": [
    "### Topic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9c13b4",
   "metadata": {},
   "source": [
    "#### Vector Space Model (VSM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0c9531",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import dok_matrix\n",
    "\n",
    "# Define function to convert Gensim corpus to a sparse pandas DataFrame\n",
    "def corpus_to_sparse_dataframe(corpus):\n",
    "    word_freq = dok_matrix((len(corpus), len(id2word)), dtype=int)\n",
    "\n",
    "    for i, doc in enumerate(corpus):\n",
    "        for word_id, freq in doc:\n",
    "            word_freq[i, word_id] = freq\n",
    "\n",
    "    dataframe = pd.DataFrame.sparse.from_spmatrix(word_freq)\n",
    "    dataframe.columns = [id2word[word_id] for word_id in range(len(id2word))]\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc989a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "VSM = corpus_to_sparse_dataframe(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6665241a",
   "metadata": {},
   "source": [
    "#### Vector Space Model & tf-idf (VSM & tf-idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2cc65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel\n",
    "\n",
    "model = TfidfModel(corpus)  # fit model\n",
    "tfidf_corpus = model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61770414",
   "metadata": {},
   "source": [
    "#### Latent Semantic Indexing (LSI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069bd7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LsiModel\n",
    "\n",
    "K = 14\n",
    "lsi_model = LsiModel(corpus, id2word=id2word, num_topics=K)\n",
    "lsi_model.print_topics(num_topics=K, num_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238253a1",
   "metadata": {},
   "source": [
    "#### Latent Semantic Indexing & tf-idf (LSI & tf-idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61217532",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LsiModel\n",
    "\n",
    "K = 14\n",
    "tfidf_lsi_model = LsiModel(tfidf_corpus, id2word=id2word, num_topics=K)\n",
    "tfidf_lsi_model.print_topics(num_topics=K, num_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc80349d",
   "metadata": {},
   "source": [
    "#### Non-Negative Matrix Factorization (NMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ccdf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/nmf_tutorial.ipynb\n",
    "from gensim.models.nmf import Nmf\n",
    "\n",
    "K = 14\n",
    "nmf_model = Nmf(corpus, id2word=id2word, num_topics=K)\n",
    "nmf_model.show_topics(num_topics=K, num_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672c7ac4",
   "metadata": {},
   "source": [
    "#### Non-Negative Matrix Factorization & tf-idf (NMF & tf-idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5fbf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/nmf_tutorial.ipynb\n",
    "from gensim.models.nmf import Nmf\n",
    "\n",
    "K = 14\n",
    "tfidf_nmf_model = Nmf(tfidf_corpus, id2word=id2word, num_topics=K)\n",
    "tfidf_nmf_model.show_topics(num_topics=K, num_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d84223f",
   "metadata": {},
   "source": [
    "#### Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "491b5844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.018*\"cell\" + 0.018*\"plant\" + 0.016*\"call\" + 0.011*\"share\" + '\n",
      "  '0.009*\"organism\" + 0.008*\"figure\" + 0.008*\"time\" + 0.007*\"system\" + '\n",
      "  '0.007*\"per\" + 0.006*\"one\"'),\n",
      " (1,\n",
      "  '0.036*\"cash\" + 0.031*\"account\" + 0.031*\"debenture\" + 0.021*\"balance\" + '\n",
      "  '0.019*\"amount\" + 0.018*\"book\" + 0.016*\"purchase\" + 0.015*\"bank\" + '\n",
      "  '0.013*\"sale\" + 0.013*\"pay\"'),\n",
      " (2,\n",
      "  '0.006*\"state\" + 0.005*\"also\" + 0.005*\"new\" + 0.004*\"war\" + 0.004*\"become\" + '\n",
      "  '0.004*\"population\" + 0.004*\"country\" + 0.004*\"animal\" + 0.004*\"many\" + '\n",
      "  '0.003*\"world\"'),\n",
      " (3,\n",
      "  '0.039*\"magnetic\" + 0.038*\"field\" + 0.028*\"vector\" + 0.026*\"ﬁeld\" + '\n",
      "  '0.022*\"direction\" + 0.021*\"axis\" + 0.017*\"velocity\" + 0.015*\"magnitude\" + '\n",
      "  '0.013*\"point\" + 0.013*\"fig\"'),\n",
      " (4,\n",
      "  '0.027*\"instruction\" + 0.019*\"memory\" + 0.016*\"register\" + 0.014*\"bit\" + '\n",
      "  '0.014*\"address\" + 0.012*\"cache\" + 0.011*\"datum\" + 0.010*\"use\" + '\n",
      "  '0.009*\"computer\" + 0.009*\"page\"'),\n",
      " (5,\n",
      "  '0.027*\"number\" + 0.018*\"two\" + 0.015*\"point\" + 0.014*\"example\" + '\n",
      "  '0.013*\"length\" + 0.011*\"equation\" + 0.011*\"etc\" + 0.009*\"find\" + '\n",
      "  '0.008*\"one\" + 0.008*\"side\"'),\n",
      " (6,\n",
      "  '0.011*\"energy\" + 0.010*\"force\" + 0.009*\"charge\" + 0.009*\"water\" + '\n",
      "  '0.008*\"body\" + 0.007*\"light\" + 0.006*\"wave\" + 0.006*\"current\" + '\n",
      "  '0.006*\"surface\" + 0.006*\"two\"')]\n"
     ]
    }
   ],
   "source": [
    "# Build LDA model\n",
    "K = 7\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=K,\n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=400,\n",
    "                                           passes=30,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "# Print the Keywords in the 3 topics\n",
    "pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b87e268b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "drop() takes from 1 to 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\DANIEL~1\\AppData\\Local\\Temp/ipykernel_13136/848851271.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#Creating Topic Distance Visualization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_notebook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgensim_models\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlda_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mid2word\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyLDAvis\\gensim_models.py\u001b[0m in \u001b[0;36mprepare\u001b[1;34m(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \"\"\"\n\u001b[0;32m    122\u001b[0m     \u001b[0mopts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_extract_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_topic_dist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py\u001b[0m in \u001b[0;36mprepare\u001b[1;34m(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency, R, lambda_step, mds, n_jobs, plot_opts, sort_topics, start_index)\u001b[0m\n\u001b[0;32m    437\u001b[0m     \u001b[0mterm_frequency\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mterm_topic_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m     topic_info = _topic_info(topic_term_dists, topic_proportion,\n\u001b[0m\u001b[0;32m    440\u001b[0m                              \u001b[0mterm_frequency\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterm_topic_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mR\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m                              n_jobs, start_index)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py\u001b[0m in \u001b[0;36m_topic_info\u001b[1;34m(topic_term_dists, topic_proportion, term_frequency, term_topic_freq, vocab, lambda_step, R, n_jobs, start_index)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[1;34m'Total'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mterm_frequency\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m         'Category': 'Default'})\n\u001b[1;32m--> 246\u001b[1;33m     default_term_info = default_term_info.sort_values(\n\u001b[0m\u001b[0;32m    247\u001b[0m         by='saliency', ascending=False).head(R).drop('saliency', 1)\n\u001b[0;32m    248\u001b[0m     \u001b[1;31m# Rounding Freq and Total to integer values to match LDAvis code:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: drop() takes from 1 to 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "#Creating Topic Distance Visualization \n",
    "pyLDAvis.enable_notebook()\n",
    "p = pyLDAvis.gensim_models.prepare(topic_model = lda_model, corpus = corpus, dictionary = id2word)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee36a6ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
