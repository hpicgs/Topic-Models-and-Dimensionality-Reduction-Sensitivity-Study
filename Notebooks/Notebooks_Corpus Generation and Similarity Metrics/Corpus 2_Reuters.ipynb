{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1079e22",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d54c2fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel Atzberger\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.4)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n",
      "C:\\Users\\Daniel Atzberger\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import operator\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import LsiModel\n",
    "\n",
    "import spacy\n",
    "from keras.datasets import reuters \n",
    "\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from','subject','re','edu','use'])\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e100718",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel Atzberger\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\reuters.py:148: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "C:\\Users\\Daniel Atzberger\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\reuters.py:149: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80ab2e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10\n",
    "\n",
    "def top_K_frequent_integers(arr):\n",
    "    # Count occurrences of each integer in the list\n",
    "    counter = Counter(arr)\n",
    "    # Get the K most common integers and their frequencies\n",
    "    most_common = counter.most_common(K)  \n",
    "    return most_common\n",
    "\n",
    "def extract_first_elements(tuple_list):\n",
    "    return [t[0] for t in tuple_list]\n",
    "\n",
    "def find_indices(main_list, check_list):\n",
    "    indices = [index for index, element in enumerate(main_list) if element in check_list]\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d76d7430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_elements_with_indices(main_list, indices):\n",
    "    return [main_list[index] for index in indices if index < len(main_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11fe1504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'? ? ? said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = reuters.get_word_index()\n",
    "reverse_word_index = dict([(value,key) for (key, value) in word_index.items()])\n",
    "decoded_train_data = []\n",
    "for j in range(len(train_data)):\n",
    "    decoded_doc = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[j]])\n",
    "    decoded_train_data.append(decoded_doc)\n",
    "\n",
    "decoded_train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7326f0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_dict = top_K_frequent_integers(list(train_labels))\n",
    "topics = extract_first_elements(topics_dict)\n",
    "indices_positions = find_indices(list(train_labels), topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5e47ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_train_data_selected = get_elements_with_indices(decoded_train_data, indices_positions)\n",
    "train_labels_selected = get_elements_with_indices(list(train_labels), indices_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63ab0660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'? ? ? said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_train_data_selected[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53dd095c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "        \n",
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2384919b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['said', 'as', 'result', 'of', 'its', 'december', 'acquisition', 'of', 'space', 'co', 'it', 'expects', 'earnings', 'per', 'share', 'in', 'of', 'to', 'dlrs', 'per', 'share', 'up', 'from', 'cts', 'in', 'the', 'company', 'said', 'pretax', 'net', 'should', 'rise', 'to', 'nine', 'to', 'mln', 'dlrs', 'from', 'six', 'mln', 'dlrs', 'in', 'and', 'rental', 'operation', 'revenues', 'to', 'to', 'mln', 'dlrs', 'from', 'mln', 'dlrs', 'it', 'said', 'cash', 'flow', 'per', 'share', 'this', 'year', 'should', 'be', 'to', 'three', 'dlrs', 'reuter']\n"
     ]
    }
   ],
   "source": [
    "data_words = list(sent_to_words(decoded_train_data_selected))\n",
    "print(data_words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f77c6446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start removing stop words\n",
      "['said', 'result', 'december', 'acquisition', 'space', 'co', 'expects', 'earnings', 'per', 'share', 'dlrs', 'per', 'share', 'cts', 'company', 'said', 'pretax', 'net', 'rise', 'nine', 'mln', 'dlrs', 'six', 'mln', 'dlrs', 'rental', 'operation', 'revenues', 'mln', 'dlrs', 'mln', 'dlrs', 'said', 'cash', 'flow', 'per', 'share', 'year', 'three', 'dlrs', 'reuter']\n"
     ]
    }
   ],
   "source": [
    "# Remove Stop Words\n",
    "print(\"Start removing stop words\")\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "print(data_words_nostops[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8b94d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing spacy\n",
      "Start lemmatizing words\n",
      "['say', 'result', 'december', 'acquisition', 'space', 'co', 'expect', 'earning', 'per', 'share', 'dlrs', 'per', 'share', 'ct', 'company', 'say', 'pretax', 'net', 'rise', 'nine', 'mln', 'dlrs', 'six', 'mln', 'dlrs', 'rental', 'operation', 'revenue', 'mln', 'dlrs', 'mln', 'dlrs', 'say', 'cash', 'flow', 'per', 'share', 'year', 'three', 'dlrs', 'reuter']\n"
     ]
    }
   ],
   "source": [
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# conda install -c conda-forge spacy-model-en_core_web_sm\n",
    "print(\"Installing spacy\")\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "print(\"Start lemmatizing words\")\n",
    "#data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "data_lemmatized = lemmatization(data_words_nostops, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "print(data_lemmatized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac7a06e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['result', 'december', 'acquisition', 'space', 'expect', 'earning', 'share', 'dlrs', 'share', 'company', 'pretax', 'rise', 'nine', 'dlrs', 'dlrs', 'rental', 'operation', 'revenue', 'dlrs', 'dlrs', 'cash', 'flow', 'share', 'year', 'three', 'dlrs', 'reuter']\n"
     ]
    }
   ],
   "source": [
    "data_lemmatized_min_length = []\n",
    "\n",
    "for sublist in data_lemmatized:\n",
    "    # Use a list comprehension to filter out strings with less than two characters\n",
    "    sublist = [word for word in sublist if len(word) > 3]\n",
    "    data_lemmatized_min_length.append(sublist)\n",
    "    \n",
    "print(data_lemmatized_min_length[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20da1541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 6), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 3), (17, 1), (18, 1), (19, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized_min_length)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized_min_length\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View \n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2fc25146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7627\n",
      "6191\n"
     ]
    }
   ],
   "source": [
    "# Number of documents\n",
    "print(len(corpus))\n",
    "\n",
    "# Size of the vocabulary\n",
    "print(len(id2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb3511ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "def sum_of_second_components(tuple_list):\n",
    "    total_sum = 0\n",
    "    for tup in tuple_list:\n",
    "        total_sum += tup[1]  # Accessing the second component of each tuple\n",
    "    return total_sum\n",
    "\n",
    "lengths = []\n",
    "for doc in corpus:\n",
    "    lengths.append(sum_of_second_components(doc))\n",
    "print(statistics.median(lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c5b667",
   "metadata": {},
   "source": [
    "### Topic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ebe4cc",
   "metadata": {},
   "source": [
    "#### Vector Space Model (VSM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598fdcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import dok_matrix\n",
    "\n",
    "# Define function to convert Gensim corpus to a sparse pandas DataFrame\n",
    "def corpus_to_sparse_dataframe(corpus):\n",
    "    word_freq = dok_matrix((len(corpus), len(id2word)), dtype=int)\n",
    "\n",
    "    for i, doc in enumerate(corpus):\n",
    "        for word_id, freq in doc:\n",
    "            word_freq[i, word_id] = freq\n",
    "\n",
    "    dataframe = pd.DataFrame.sparse.from_spmatrix(word_freq)\n",
    "    dataframe.columns = [id2word[word_id] for word_id in range(len(id2word))]\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14485010",
   "metadata": {},
   "outputs": [],
   "source": [
    "VSM = corpus_to_sparse_dataframe(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6da01d1",
   "metadata": {},
   "source": [
    "#### VSM and tf-idf (VSM & tf-idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd622dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel\n",
    "\n",
    "model = TfidfModel(corpus)  # fit model\n",
    "tfidf_corpus = model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de36ba1d",
   "metadata": {},
   "source": [
    "#### Latent Semantic Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d28c3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LsiModel\n",
    "\n",
    "K = 10\n",
    "lsi_model = LsiModel(corpus, id2word=id2word, num_topics=K)\n",
    "lsi_model.print_topics(num_topics=K, num_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41b5b16",
   "metadata": {},
   "source": [
    "#### Latent Semantic Indexing and tf-idf (LSI & tf-idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8394affc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LsiModel\n",
    "\n",
    "K = 10\n",
    "tfidf_lsi_model = LsiModel(tfidf_corpus, id2word=id2word, num_topics=K)\n",
    "tfidf_lsi_model.print_topics(num_topics=K, num_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac0caa4",
   "metadata": {},
   "source": [
    "#### Non-Negative Matrix Factorization (NMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dabaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/nmf_tutorial.ipynb\n",
    "from gensim.models.nmf import Nmf\n",
    "\n",
    "K = 10\n",
    "nmf_model = Nmf(corpus, id2word=id2word, num_topics=K)\n",
    "nmf_model.show_topics(num_topics=K, num_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48235f6",
   "metadata": {},
   "source": [
    "#### Non-Negative Matrix Factorization and tf-idf (NMF & tf-idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6bb582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/nmf_tutorial.ipynb\n",
    "from gensim.models.nmf import Nmf\n",
    "\n",
    "K = 10\n",
    "tfidf_nmf_model = Nmf(tfidf_corpus, id2word=id2word, num_topics=K)\n",
    "tfidf_nmf_model.show_topics(num_topics=K, num_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f4eec6",
   "metadata": {},
   "source": [
    "#### Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04663d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "K = 10\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=K,\n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=400,\n",
    "                                           passes=30,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "# Print the Keywords in the 3 topics\n",
    "pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a260ab6",
   "metadata": {},
   "source": [
    "#### Miscallenous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db57cfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b955ee15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.print_topics(num_topics=20, num_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff5e22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model[corpus[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e544bf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_top = []\n",
    "for i in tqdm(range(len(corpus))):\n",
    "    doc_top.append(model[corpus[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803b0a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for i in range(len(corpus)):\n",
    "    doc_top = []\n",
    "    doc_topics = model[corpus[i]]\n",
    "    for j in range(K):\n",
    "        doc_top.append(doc_topics[j][1])\n",
    "    rows.append(doc_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635a3cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DTM = pd.DataFrame(rows)\n",
    "#document_topic_matrix_sourcecode[\"identifier\"] = df_sourcecode.iloc[:,0].tolist()\n",
    "DTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2bea3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = train_labels_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b51709c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality reduction: UMAP\n",
    "import umap\n",
    "import time\n",
    "\n",
    "time_start = time.time()\n",
    "umap = umap.UMAP(n_components = 2, n_neighbors = 10, min_dist = 0.1)\n",
    "umap_results = umap.fit_transform(DTM)\n",
    "\n",
    "print ('UMAP done! Time elapsed: {} seconds'.format(time.time()-time_start))\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Create the figure\n",
    "fig = plt.figure( figsize=(8,8) )\n",
    "ax = fig.add_subplot(1, 1, 1, title='UMAP' )\n",
    "# Create the scatter\n",
    "ax.scatter(\n",
    "    x=umap_results[:,0], \n",
    "    y=umap_results[:,1], \n",
    "    c=Y, \n",
    "    cmap=plt.cm.get_cmap('Paired'), \n",
    "    alpha=1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93106245",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
